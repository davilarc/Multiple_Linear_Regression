---
title: "L13Ex_ToolLifeEx_Rick_Davila"
author: "Rick Davila"
date: "5/29/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

#### Perform data housekeeping - upload, name columns, display to make sure it reads properly, etc. 

```{r setup, include=TRUE, results="asis"}
knitr::opts_chunk$set(echo = TRUE)

Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jdk-14.0.1') # for 64-bit version
library(rJava)


library(e1071)
library("xlsx")
library(xtable)
# for laptop 2 -- result of reloading java JDK version


rm(list = ls())

# load data
Ex81 <- read.xlsx(
  "data-ex-8-1.xlsx",
  sheetIndex = 1, sheetName=NULL, rowIndex=NULL, 
  startRow=NULL, endRow=NULL, colIndex= c(1,2,3,4),
  as.data.frame=TRUE, header=TRUE, colClasses=NA, 
  keepFormulas=FALSE, encoding="unknown")

# Give labels to data columns
names(Ex81) <- c("Obs", 
                 "y_i",
                 "x_1",
                 "Tool_Type")
attach(Ex81)

# Output data to make sure it reads properly
out <- as.data.frame(c(Ex81))
colnames(out) <- c("Obs", 
                 "y_i",
                 "x_1",
                 "Tool_Type")
tab <- (xtable(out, digits=c(0,0,2,0,0)))
print(tab, type="html")

# Output data structure and dimensions
str(Ex81)
dim(Ex81)

```

## Example 8.1 (p.262-264) ###
#### Create scatterplot to look at the data - all points look the same

```{r}

plot(x_1,y_i, main = "lathe speed (x1) versus tool life (y)",
     xlab = "Speed, x_1",
     ylab = "Tool Life, y")

```

#### Create scatterplot - distinguish between Tool Type using points() command to make Tool Type A green and Tool Type B red.

```{r}

plot(x_1,y_i, main = "lathe speed (x1) versus tool life (y)",
     xlab = "Speed, x_1",
     ylab = "Tool Life, y")

# in the for loop, use points() function to distinguish between tool types
for (pltpts in seq(1:length(y_i))){

  if (Tool_Type[pltpts] == "A") {
    points(x_1[pltpts], y_i[pltpts], col = "green")} 
  else if (Tool_Type[pltpts] == "B") {
    points(x_1[pltpts], y_i[pltpts], col = "red")}
}

legend("topright", legend=c("A", "B"),
       col=c("green", "red"),
       pch=c(1,1))

```

#### Define indicator variables for Tool Type and add to dataframe

```{r results="asis"}

x_2 <- vector()

for (i in seq(1:length(y_i))){

  if (Tool_Type[i] == "A") {
    x_2[i] = 1} 
  else if (Tool_Type[i] == "B") {
    x_2[i] = 0}
}

Ex81$x_2 <- c(x_2)

# Output data to make sure it reads properly
out <- as.data.frame(c(Ex81))
colnames(out) <- c("Obs", 
                 "y_i",
                 "x_1",
                 "Tool_Type",
                 "x_2")
tab <- (xtable(out, digits=c(0,0,2,0,0,0)))
print(tab, type="html")

```

#### Create linear model; compare to Table 8.2 on p. 264

```{r results="asis"}

model.81 <- lm(y_i ~ x_1 + x_2)

print(xtable(summary(model.81),
             digits=c(0,3,3,2,3),
             display=c("s","f","f","f","g"))
      ,type="html")
print(xtable(anova(model.81),
             digits=c(0,0,3,3,3,3),
             display=c("s","d","f","f","f","g"))
      ,type="html")

```

The least squares fit equation is:
$$\hat{y}= `r model.81$coefficients[1]` + 
(`r model.81$coefficients[2]`)x_1 + 
(`r model.81$coefficients[3]`)x_2 $$

$$ R^2 = `r summary(model.81)$r.squared` $$
$$ R^2_{adj} = `r summary(model.81)$adj.r.squared` $$
$$F_0 = `r summary(model.81)$fstatistic[1]` $$

#### Check model adequacy with residual plots

```{r}

# fitted values (y_hat) versus R-student residuals (t_i)
plot(model.81$fitted.values, rstudent(model.81),
     main = "fitted values (y_hat) versus r-student residuals (t_i)",
     xlab = "fitted values, y_hat",
     ylab = "R-Student Residuals, t_i")

for (pltpts in seq(1:length(y_i))){

  if (Tool_Type[pltpts] == "A") {
    points(model.81$fitted.values[pltpts],
           rstudent(model.81)[pltpts],
           col = "green")} 
  else if (Tool_Type[pltpts] == "B") {
    points(model.81$fitted.values[pltpts],
           rstudent(model.81)[pltpts],
           col = "red")}
}

legend("topright", legend=c("A", "B"),
       col=c("green", "red"),
       pch=c(1,1))
abline(0,0,col="gray")

# speed versus rstudent residuals
plot(x_1, rstudent(model.81),
     main = "speed (x_1)  versus r-student residuals (t_i)",
     xlab = "speed, x_1",
     ylab = "R-Student Residuals, t_i")
abline(0,0,col="gray")

# speed versus rstudent residuals
plot(x_2, rstudent(model.81),
     main = "tool type (x_2)  versus r-student residuals (t_i)",
     xlab = "tool type (x_2",
     ylab = "R-Student Residuals, t_i")
abline(0,0,col="gray")

#Assess normality of the observations as a whole using the residuals. 
qqnorm(model.81$residuals,main="normal QQ plot of residuals")
qqline(model.81$residuals)

#Assess normality of the observations as a whole using the r-student residuals. 
qqnorm(rstudent(model.81),main="normal QQ plot of rstudent residuals")
qqline(rstudent(model.81))

```

No issues with the normal probability plot.
Constant variance with the fitted values and regression variables versus residuals; the seems to be no issues with the model fits.

## Example 8.2 (p. 267-268)
#### Fit a model were ToolType is expected to influence both slope and intercept; compare the two models using the Partial F-test
#### Add column to dataframe for Interaction

```{r results="asis"}

x_12 <- x_1 * x_2
Ex81$x_12 <- x_12

Ex81$x_12 <- c(x_12)

# Output data to make sure it reads properly
out <- as.data.frame(c(Ex81))
colnames(out) <- c("Obs", 
                 "y_i",
                 "x_1",
                 "Tool_Type",
                 "x_2",
                 "x_12")
tab <- (xtable(out, digits=c(0,0,2,0,0,0,0)))
print(tab, type="html")

```

#### Create full model; compare to p. 267

```{r}

model.82 <- lm(y_i ~ x_1 + x_2 + x_12)

summary(model.82)

```

The least squares fit equation is:
$$\hat{y}= `r model.82$coefficients[1]` + 
(`r model.82$coefficients[2]`)x_1 + 
(`r model.82$coefficients[3]`)x_2 +
(`r model.82$coefficients[4]`)x_1x_2 $$

$$ R^2 = `r summary(model.82)$r.squared` $$
$$ R^2_{adj} = `r summary(model.82)$adj.r.squared` $$
$$F_0 = `r summary(model.82)$fstatistic[1]` $$

#### Calculate SS and df for Regression and Residual for full model; compare to Table 8.3, p. 268 

```{r}

x <- anova(model.82)

SS_Reg <- sum(x$`Sum Sq`[1:3])
df_Reg <- sum(x$Df[1:3])

SS_Res <- x$`Sum Sq`[4]
df_Res <- x$Df[4]
  
```

Sum of Squares (SS) Regression: $`r SS_Reg`$ and Degrees of Freedom (df) Regression: $`r df_Reg`$

Sum of Squares (SS) Residual: $`r SS_Res`$ and Degrees of Freedom (df) Residual: $`r df_Res`$

#### Test significance of interaction term using Partial F-test. Use alpha = 0.05 as the significance level; compare to values on p. 268

```{r results="asis"}

# full model including interaction term
model.full <- lm(y_i ~ x_1 + x_2 + x_12)

# reduced model excluding interaction term
model.reduced <- lm(y_i ~ x_1 + x_2)

# anova -- comparision of reduced to full model
anova(model.reduced, model.full)

# F crit
alpha <- 0.05
df_SS_R <- anova(model.reduced, model.full)$'Df'[2]
df_SS_Res <- anova(model.reduced, model.full)$'Res.Df'[2]

F_crit <- qf(1-alpha,df_SS_R,df_SS_Res)

F_0 <- anova(model.reduced, model.full)$'F'[2]
p_value <-  anova(model.reduced, model.full)$'Pr(>F)'[2]

```

The appropriate hypotheses are
$$ H_0:\beta_3=0, H_1:\beta_3 \ne 0 $$

To test $$H_0:\beta_3=0$$ form the test statistic
$$ F_0 = \frac{SS_{reg}(\beta_3|\beta_2,\beta_1,\beta_0)/1}{MS_{Res}} = \frac{`r anova(model.reduced, model.full)$'Sum of Sq'[2]`}{`r anova(model.full)$'Mean Sq'[4]`}=`r anova(model.reduced, model.full)$'F'[2]`$$
and since $F_{`r alpha`,`r df_SS_R`,`r df_SS_Res`} = `r F_crit`$, we have $F_0 \lt F_{`r alpha`,`r df_SS_R`,`r df_SS_Res`}$ and a p-value of $P = `r p_value`$ which is greater than the $0.05$ threshold.  We don't reject the null hypothesis and conclude that the interaction term is not significant.

#### Test hypothesis that the regression lines for each tool type are identical; compare to p. 268.

```{r results="asis"}

# full model including interaction term
model.full <- lm(y_i ~ x_1 + x_2 + x_12)

# reduced model excluding interaction term
model.reduced <- lm(y_i ~ x_1)

# anova -- comparision of reduced to full model
anova(model.reduced, model.full)

# F crit
alpha <- 0.05
df_SS_R <- anova(model.reduced, model.full)$'Df'[2]
df_SS_Res <- anova(model.reduced, model.full)$'Res.Df'[2]

F_crit <- qf(1-alpha,df_SS_R,df_SS_Res)


```

To test the hypothesis that the two regression lines are identical, 
$$H_0: \beta_2=\beta_3=0 $$ use the statistic 
$$ F_0 = \frac{SS_R(\beta_2,\beta_3|\beta_{1},\beta_{0}/2}{MS_{Res}}=\frac{`r anova(model.reduced, model.full)$'Sum of Sq'[2]`/`r anova(model.reduced, model.full)$'Df'[2]`}{`r anova(model.full)$'Mean Sq'[4]`}=`r anova(model.reduced, model.full)$'F'[2]` $$
and since $F_{`r alpha`,`r df_SS_R`,`r df_SS_Res`} = `r F_crit`$, we have $F_0 \gt F_{`r alpha`,`r df_SS_R`,`r df_SS_Res`}$ and also the P statistic is $P=`r anova(model.reduced, model.full)$'Pr(>F)'[2]`$.  We reject the null hypothesis and conclude that the two regression lines are not identical.